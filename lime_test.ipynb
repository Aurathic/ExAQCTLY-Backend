{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd06b20c-33bb-4df6-b25b-c4247456c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run lime.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233e4b2a-055b-45ec-9d22-f921f83651f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom as dicom\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4594aac1-fa36-4d5e-8d30-1ac19f458f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-09 07:44:44.230346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-09 07:44:46.991507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tensorflow.keras.utils import load_img\n",
    "\n",
    "import anvil.server, anvil.media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6846c5-eca7-418e-93d4-465fdd22208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, json, io\n",
    "\n",
    "from lime import lime_image\n",
    "import skimage.segmentation as segmentation\n",
    "\n",
    "import anvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ca1757-d2c2-45f8-97f9-76abcdfd5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(inp, title=None):\n",
    "    \"\"\"denormalize inp\"\"\"\n",
    "    # inp = inp.numpy().transpose((1, 2, 0))\n",
    "    # Inverse of the initial normalization operation.\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "    # plt.figure()\n",
    "    # plt.imshow(inp)\n",
    "    # if title is not None:\n",
    "    #     plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b2ba2d-397a-4594-8d55-5a93160d774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain(image, batch_predict, num_samples=1000):\n",
    "    \"\"\"Returns a byte array representing the image + explanations\n",
    "    in PNG format.\"\"\"\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    # print(\"try to explain\")\n",
    "    # print(image)\n",
    "    explanation = explainer.explain_instance(image, \n",
    "                                         batch_predict, # classification function\n",
    "                                         top_labels=2, \n",
    "                                         hide_color=0, \n",
    "                                         num_samples=num_samples) # number of images that will be sent to classification function\n",
    "    # print(\"explained\")\n",
    "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=2, hide_rest=False)\n",
    "    # print(temp, mask)\n",
    "    temp = denormalize(temp) * 255\n",
    "    # print(temp)\n",
    "    image_boundary = segmentation.mark_boundaries(temp, mask)\n",
    "    # print((\"image_boundary\", image_boundary))\n",
    "    # np_array_for_explain = transform_img(filename).numpy()\n",
    "    img = Image.fromarray((image_boundary).astype('uint8'), mode='RGB')\n",
    "    # print(img)\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    img.save(img_byte_arr, format='PNG')\n",
    "    img_byte_arr = img_byte_arr.getvalue()\n",
    "    return img_byte_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019b291b-0ca5-462b-ae5c-c9ea0bb81249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Connected to \"Published\" as SERVER\n"
     ]
    }
   ],
   "source": [
    "anvil.server.connect(\"server_Z2TJZAQV66QSSNBWMV4L4S4A-SDDDJFHQEVJ3JDQW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4a320a-e6d4-4f00-8f0a-b52f71f3d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Pennylane\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dede9e9-217a-4583-b993-e7a2c1f71f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4                # Number of qubits\n",
    "step = 0.0004               # Learning rate\n",
    "batch_size = 4              # Number of samples for each training step\n",
    "num_epochs = 3              # Number of training epochs\n",
    "q_depth = 6                 # Depth of the quantum circuit (number of variational layers)\n",
    "gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\n",
    "q_delta = 0.01              # Initial spread of random quantum weights\n",
    "start_time = time.time()    # Start of the computation timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a66606-2936-4456-aa40-40a66c642ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5836e4-a5f1-42b2-afc6-844751b1698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c810cb6d-9924-4cfe-b941-88363e6d2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a1f4208-6d0d-465b-9c36-9cef7918ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e720a9c-1f4c-4f36-aca8-ede4232eb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.pre_net = nn.Linear(512, n_qubits)\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "        self.post_net = nn.Linear(n_qubits, 2)\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain the input features for the quantum circuit\n",
    "        # by reducing the feature dimension from 512 to 4\n",
    "        pre_out = self.pre_net(input_features)\n",
    "        q_in = torch.tanh(pre_out) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        for elem in q_in:\n",
    "            q_out_elem = quantum_net(elem, self.q_params).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the two-dimensional prediction from the postprocessing layer\n",
    "        return self.post_net(q_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b241ed80-7328-48d7-80cc-e4ffd2a2ad02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_hybrid = torchvision.models.resnet18(pretrained=True)\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "# model_hybrid = torchvision.models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model_hybrid = resnet18() # weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for param in model_hybrid.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "# Notice that model_hybrid.fc is the last layer of ResNet18\n",
    "model_hybrid.fc = DressedQuantumNet()\n",
    "\n",
    "# Use CUDA or CPU according to the \"device\" object.\n",
    "model_hybrid = model_hybrid.to(device)\n",
    "\n",
    "model_hybrid.load_state_dict(torch.load(\"../model_hybrid_sim.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80634777-d12d-4bee-8a03-629a8116ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_img(img): # also reads image\n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            # transforms.RandomResizedCrop(224),     # uncomment for data augmentation\n",
    "            # transforms.RandomHorizontalFlip(),     # uncomment for data augmentation\n",
    "            # transforms.ToPILImage(), # convert to PIL image\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            # Normalize input channels using mean values and standard deviations of ImageNet.\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            # transforms.ToPILImage(), # convert to PIL image\n",
    "        ]\n",
    "    )\n",
    "    image = dicom.dcmread(img) # unsafe?\n",
    "    arr = image.pixel_array\n",
    "    image_2d = arr.astype(float)\n",
    "    image_2d = image_2d / image.LargestImagePixelValue * 255\n",
    "    image_2d = PIL.Image.fromarray(image_2d) # , mode=\"L\")\n",
    "    image_2d = image_2d.convert(\"RGB\")\n",
    "    # image_2d = transform_img(image_2d)\n",
    "    image_2d = data_transforms(image_2d)\n",
    "    return image_2d\n",
    "    # return data_transforms(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080dfa8e-890b-443e-bdbb-4128aeb4c3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_predict(model, img):\n",
    "    # print(\"Type of img:\", type(img))\n",
    "    # print(img.shape)\n",
    "    # image_2d = transform_img(img)\n",
    "    image_2d = img\n",
    "    # image_2d = torch.Tensor(image_2d)\n",
    "    with torch.no_grad():\n",
    "        inputs = image_2d.to(device)\n",
    "        # print(inputs)\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        prob = F.softmax(outputs, dim=1)\n",
    "        prob = prob[0,1].item() # always get prob of cancer, which is the 1 label\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # rint(preds)\n",
    "        #     for j in range(inputs.size()[0]):\n",
    "        # ground_truth = labels[j].item() # \"MALIGNANT\" if labels[j].item() else \"BENIGN\"\n",
    "        predicted = preds.item()\n",
    "        # y_true.append(ground_truth)\n",
    "        # y_pred.append(predicted)\n",
    "        # print(predicted)\n",
    "        # print(prob[0,1].item()) # always get prob of cancer, which is the 1 label\n",
    "    return predicted, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6bfd481-c195-43ed-8656-8230959a94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_transform(model, img):\n",
    "    return model_predict(transform_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b7a3174-716d-4fd3-8ed0-3a2ddb655aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(np_array):\n",
    "    # hacky - lime expects x,y,3 and classifier expects 3,x,y\n",
    "    # rotate things around\n",
    "    np_array = [torch.tensor(np.transpose(a, (2, 0, 1))) for a in np_array] # array of arrays?\n",
    "    return [model_predict(model_hybrid, a) for a in np_array]\n",
    "    # convert np_array to tensor\n",
    "    img_tensor = torch.tensor(np_array)\n",
    "    # img_tensor = img_tensor.apply_(transform_img)\n",
    "    model = model_hybrid\n",
    "    with torch.no_grad():\n",
    "        inputs = img_tensor.to(device)\n",
    "        # print(inputs)\n",
    "        # inputs = inputs.unsqueeze(0)\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs)\n",
    "        prob = F.softmax(outputs, dim=1)\n",
    "        # need to always get the cancer prob\n",
    "        prob = prob[0,1].item() # always get prob of cancer, which is the 1 label\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # rint(preds)\n",
    "        #     for j in range(inputs.size()[0]):\n",
    "        # ground_truth = labels[j].item() # \"MALIGNANT\" if labels[j].item() else \"BENIGN\"\n",
    "        predicted = preds.item()\n",
    "        # y_true.append(ground_truth)\n",
    "        # y_pred.append(predicted)\n",
    "        # print(predicted)\n",
    "        # print(prob[0,1].item()) # always get prob of cancer, which is the 1 label\n",
    "    return predicted, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71cff576-b49a-4e54-8023-b43628510328",
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def classify(file, give_explanation=True, num_samples=100): \n",
    "    # Classify\n",
    "    # print(type(file))\n",
    "    with anvil.media.open(file) as filename:\n",
    "        # print(type(filename))\n",
    "        ti = transform_img(filename)\n",
    "        # print(ti)\n",
    "        _, pred_cancer = model_predict(model_hybrid, ti)\n",
    "    # model.predict\n",
    "    # Give explanation\n",
    "        if give_explanation:\n",
    "            # with anvil.media.open(file) as filename:\n",
    "                # img = load_img(filename)\n",
    "            # print(filename)\n",
    "            # np_array_for_explain = transform_img(filename).numpy()\n",
    "            np_array_for_explain = ti.numpy()\n",
    "            np_array_for_explain = np.transpose(np_array_for_explain, (1, 2, 0))\n",
    "            # print(np_array_for_explain)\n",
    "            img_byte_arr = explain(np_array_for_explain, batch_predict, num_samples=100)\n",
    "            anvil_img = anvil.BlobMedia(content_type=\"image/png\", content=img_byte_arr, name=\"explain.png\")\n",
    "            return pred_cancer, anvil_img\n",
    "        return pred_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb6b4aeb-2aed-4d48-b9bc-e522563e3171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torchvision.io\n",
    "@anvil.server.callable\n",
    "def to_png(file):\n",
    "    with anvil.media.open(file) as filename:\n",
    "        # print(filename)\n",
    "        image = dicom.dcmread(filename)\n",
    "        arr = image.pixel_array\n",
    "        image_2d = arr.astype(float)\n",
    "        image_2d = image_2d / image.LargestImagePixelValue * 255\n",
    "        image_2d = PIL.Image.fromarray(image_2d) # , mode=\"L\")\n",
    "        image_2d = image_2d.convert(\"RGB\")\n",
    "        # print(image_2d)\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image_2d.save(img_byte_arr, format='PNG')\n",
    "        img_byte_arr = img_byte_arr.getvalue()\n",
    "        # image_2d = transform_img(image_2d) # tensor\n",
    "        # print(image_2d)\n",
    "        # image_2d = image_2d.to(torch.uint8)\n",
    "        # print(image_2d)\n",
    "        # image_2d = torchvision.io.encode_png(image_2d)\n",
    "        # print(image_2d)\n",
    "        # image_2d.seek(0)\n",
    "        # image_2d = image_2d.read()\n",
    "        # is a tensor\n",
    "        # buff = io.BytesIO()\n",
    "        # torch.save(image_2d, buff)\n",
    "        # buff.seek(0)\n",
    "        # print(buff.read())\n",
    "        # buff.seek(0)\n",
    "        # img_byte_arr = explain(pill_transf(img), batch_predict, num_samples=100)\n",
    "        # anvil_img = anvil.BlobMedia(content_type=\"image/png\", content=img_byte_arr, name=\"explain.png\")\n",
    "        anvil_img = anvil.BlobMedia(content_type=\"image/png\", content=img_byte_arr, name=\"preview.png\")\n",
    "        return anvil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b87319-5f19-450c-a53d-236b724b1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"../tciaDownload/1.3.6.1.4.1.9590.100.1.2.100018879311824535125115145152454291132/1-1.dcm\"\n",
    "t=transform_img(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89df330c-3402-4060-8718-f75524385636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.4761468768119812\n"
     ]
    }
   ],
   "source": [
    "predicted, prob = model_predict(model_hybrid, t)\n",
    "print(predicted)\n",
    "print(prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
